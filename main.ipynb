{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 example scales:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('attractive, unattractive**',\n",
       "  {'adjectives': ['plain', 'unattractive', 'ugly']}),\n",
       " ('audible, inaudible**', {'adjectives': ['quiet', 'inaudible', 'silent']}),\n",
       " ('big, little**',\n",
       "  {'adjectives': ['small', 'smaller', 'midget', 'minute', 'tiny', 'micro']})]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## parse scales datasets and add all scales to 'data' which is the main var we're going to be working with\n",
    "data = {}\n",
    "scale_datanames = ['demelo', 'crowd', 'wilkinson']\n",
    "\n",
    "# for every dataset there will be a different key in 'data' (e.g. data['demelo'])\n",
    "# for every scale in a dataset (e.g. 'attractive, unattractive**' in demelo), there will be a key with that name in data['demelo'],\n",
    "# and the value is an object whose value at \"adjectives\" will be the list of adjectives in that scale, ordered by intensity (e.g. \"[pretty, beautiful, gorgeous]\").\n",
    "for dataset_name in scale_datanames:\n",
    "    data[dataset_name] = {}\n",
    "    file_path = f\"data\\{dataset_name}.txt\"\n",
    "    file_data = open(file_path, 'r').read()\n",
    "    scales = file_data.split(\"=== \")[1:]\n",
    "    for scale in scales:\n",
    "        scale_data = scale.split('\\n')\n",
    "        scale_name = scale_data[0]\n",
    "        scale_adjectives = scale_data[1:-1]\n",
    "        scale_adjectives = [x.split(' || ')[0] for x in scale_adjectives] # remove ties\n",
    "        data[dataset_name][scale_name] = { \"adjectives\": scale_adjectives }\n",
    "\n",
    "# example entries in demelo dataset\n",
    "print(\"3 example scales:\")\n",
    "list(data[\"demelo\"].items())[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Found cached dataset imdb (C:/Users/alonmizrahi/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0)\n",
      "100%|██████████| 3/3 [00:00<00:00, 20.49it/s]\n",
      "Found cached dataset rotten_tomatoes (C:/Users/alonmizrahi/.cache/huggingface/datasets/rotten_tomatoes/default/1.0.0/40d411e45a6ce3484deed7cc15b82a53dad9a72aafd9f86f8f227134bec5ca46)\n",
      "100%|██████████| 3/3 [00:00<00:00, 93.73it/s]\n",
      "Found cached dataset yelp_review_full (C:/Users/alonmizrahi/.cache/huggingface/datasets/yelp_review_full/yelp_review_full/1.0.0/e8e18e19d7be9e75642fc66b198abadb116f73599ec89a69ba5dd8d1e57ba0bf)\n",
      "100%|██████████| 2/2 [00:00<00:00,  6.53it/s]\n",
      "Found cached dataset cc_news (C:/Users/alonmizrahi/.cache/huggingface/datasets/cc_news/plain_text/1.0.0/e3d5612f02fe5f11826a0d9614328b1772e27e5d685f4ec438e7f768e4581734)\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flattening datasets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:03<00:00,  1.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shuffling datasets...\n",
      "dataset 'reviews' len: 50000\n",
      "dataset 'news' len: 50000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "## download datasets of 2 categories: reviews and news.\n",
    "import datasets\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "reviews_datasets = []\n",
    "news_datasets = []\n",
    "\n",
    "# download datasets\n",
    "reviews_datanames = [\"imdb\", \"rotten_tomatoes\", \"yelp_review_full\"] # movie / business reviews\n",
    "news_datanames = [\"cc_news\"]\n",
    "\n",
    "for name in reviews_datanames:\n",
    "    reviews_datasets.append(datasets.load_dataset(name))\n",
    "for name in news_datanames:\n",
    "    news_datasets.append(datasets.load_dataset(name))\n",
    "\n",
    "all_datasets = reviews_datasets + news_datasets\n",
    "\n",
    "# flatten dataset splits (train/test/validation) into one set, taking only the 'text' column\n",
    "print(f\"flattening datasets...\")\n",
    "for i in tqdm(range(len(all_datasets))):\n",
    "    flattened = []\n",
    "    for subset_name in all_datasets[i]:\n",
    "        flattened += all_datasets[i][subset_name][\"text\"]\n",
    "        \n",
    "    all_datasets[i] = flattened\n",
    "\n",
    "all_datanames = [\"reviews\", \"news\"]\n",
    "# make sure we have the same number of examples from 'reviews' and 'news' datasets (50k examples each)\n",
    "all_datasets = [all_datasets[0][:15000] + all_datasets[1][:10000] + all_datasets[2][:25000], all_datasets[3][:50000]] \n",
    "\n",
    "# shuffle\n",
    "print(\"shuffling datasets...\")\n",
    "random.seed(9001)\n",
    "for i, dataset in enumerate(all_datasets):\n",
    "    random.shuffle(dataset)\n",
    "    print(f\"dataset '{all_datanames[i]}' len: {len(dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading finetuned model from \"bert_finetuned_on_cola/model\"...\n",
      "evaluation:\n",
      "('Hello, how are you?', 'correct')\n",
      "('Hello are.', 'incorrect')\n",
      "('You are very nice', 'correct')\n",
      "('Very nice you.', 'incorrect')\n",
      "('The movie was horrible', 'correct')\n",
      "('The was movie horrible', 'incorrect')\n"
     ]
    }
   ],
   "source": [
    "## finetune BERT on cola\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import BertForSequenceClassification, BertTokenizer, Trainer, TrainingArguments\n",
    "\n",
    "if os.path.exists(\"bert_finetuned_on_cola/model\") and os.path.exists(\"bert_finetuned_on_cola/tokenizer\"):\n",
    "  print(f'loading finetuned model from \"bert_finetuned_on_cola/model\"...')\n",
    "  cola_model = BertForSequenceClassification.from_pretrained(\"bert_finetuned_on_cola/model\").to(\"cuda\")\n",
    "  cola_tokenizer = BertTokenizer.from_pretrained(\"bert_finetuned_on_cola/tokenizer\")\n",
    "else:\n",
    "  model_name = \"bert-base-uncased\"\n",
    "  print(f\"loading and finetuning {model_name} on cola dataset...\")\n",
    "  cola_model = BertForSequenceClassification.from_pretrained(model_name).to(\"cuda\")\n",
    "  cola_tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "\n",
    "  cola = datasets.load_dataset('linxinyuan/cola')\n",
    "  cola_tokenized = cola.map(lambda examples: cola_tokenizer(examples['text'], padding=True, truncation=True), batched=True, batch_size=16)\n",
    "  cola_tokenized = cola_tokenized.rename_column(\"label\", \"labels\")\n",
    "  cola_tokenized.set_format(\"torch\")\n",
    "\n",
    "  def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "\n",
    "    accuracy = np.mean(preds == labels)\n",
    "    true_positives = np.sum((labels == 1) & (preds == 1))\n",
    "    false_positives = np.sum((labels == 0) & (preds == 1))\n",
    "    false_negatives = np.sum((labels == 1) & (preds == 0))\n",
    "      \n",
    "    precision = true_positives / (true_positives + false_positives + 1e-9)\n",
    "    recall = true_positives / (true_positives + false_negatives + 1e-9)\n",
    "    f1 = 2 * (precision * recall) / (precision + recall + 1e-9)\n",
    "    \n",
    "    # we're mostly interested in precision, that is, reducing the number of times we predict incorrect sentences as correct\n",
    "    return {'accuracy': accuracy, 'precision': precision, 'recall': recall, 'f1': f1}\n",
    "\n",
    "  # Configure training arguments\n",
    "  training_args = TrainingArguments(\n",
    "      output_dir=\"./bert_finetuned_on_cola\",\n",
    "      evaluation_strategy=\"epoch\",\n",
    "      num_train_epochs=8,\n",
    "      per_device_train_batch_size=16,\n",
    "      per_device_eval_batch_size=16,\n",
    "  )\n",
    "\n",
    "  # Create Trainer instance\n",
    "  trainer = Trainer(\n",
    "      model=cola_model,\n",
    "      args=training_args,\n",
    "      train_dataset=cola_tokenized[\"train\"],\n",
    "      eval_dataset=cola_tokenized[\"test\"],\n",
    "      compute_metrics=compute_metrics,\n",
    "      tokenizer=cola_tokenizer\n",
    "  )\n",
    "\n",
    "  trainer.train()\n",
    "\n",
    "  # save model and tokenizer\n",
    "  cola_model.save_pretrained('bert_finetuned_on_cola/model')\n",
    "  cola_tokenizer.save_pretrained('bert_finetuned_on_cola/tokenizer')\n",
    "\n",
    "\n",
    "# evaluate\n",
    "print(\"evaluation:\")\n",
    "input = [\"Hello, how are you?\",\n",
    "         \"Hello are.\",\n",
    "         \"You are very nice\",\n",
    "         \"Very nice you.\",\n",
    "         \"The movie was horrible\",\n",
    "         \"The was movie horrible\"]\n",
    "tokens = cola_tokenizer(input, return_tensors=\"pt\", padding=True).to('cuda')\n",
    "\n",
    "cola_model.eval()\n",
    "with torch.no_grad():\n",
    "  output = cola_model(**tokens)\n",
    "\n",
    "logits = output.logits.cpu().numpy()\n",
    "\n",
    "predictions = np.argmax(logits, axis=-1)\n",
    "predictions_text = [\"correct\" if x == 1 else \"incorrect\" for x in predictions]\n",
    "for item in zip(input, predictions_text):\n",
    "  print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pretty', 'beautiful', 'gorgeous']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# helper function to flatten adjective list with potential ties\n",
    "# ['pretty || beautiful', 'gorgeous'] -> ['pretty', 'beautiful', 'gorgeous']\n",
    "def extract_adjectives(adjs):\n",
    "    return [item for sublist in [[x] if \" || \" not in x else x.split(\" || \") for x in adjs] for item in sublist]\n",
    "\n",
    "extract_adjectives(['pretty || beautiful', 'gorgeous'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parsing sentences from 'reviews' dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [03:33<00:00, 233.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parsing sentences from 'news' dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [04:56<00:00, 168.37it/s]\n"
     ]
    }
   ],
   "source": [
    "## parse sentences - add them to each adjective\n",
    "import nltk\n",
    "\n",
    "MAX_SENTENCES_PER_ADJ = 100\n",
    "for dataset_index, dataset in enumerate(all_datasets):\n",
    "    dataset_type = all_datanames[dataset_index]\n",
    "    print(f\"parsing sentences from '{dataset_type}' dataset...\")\n",
    "\n",
    "    for text in tqdm(dataset):\n",
    "        text = text.lower().replace(\"\\\\\", \"\").replace(\"<br />\", \"\").replace(\"\\\\n\\\\n\", \". \").replace(\"\\\\n\", \". \").replace(\"\\n\", \". \")\n",
    "        sentences = nltk.sent_tokenize(text)\n",
    "            \n",
    "        # for each sentence check all scales\n",
    "        for sentence in sentences:\n",
    "            words = nltk.word_tokenize(sentence)\n",
    "            if len(words) < 5 or len(words) > 35: continue # not too short, not too long\n",
    "\n",
    "            for scales_dataset in data.values(): # demelo / crowd / wilkinson\n",
    "                for scale in scales_dataset.values(): # '{ \"adjectives\": [\"nearby || near\", \"close\"] }'\n",
    "                    if not dataset_type in scale: scale[dataset_type] = {} # create an obj for 'reviews' or 'news' if doesn't exist\n",
    "                    adjs = extract_adjectives(scale['adjectives']) # flatten list of adjectives as it might contain ties\n",
    "                    for adj in adjs:\n",
    "                        if adj in scale[dataset_type] and len(scale[dataset_type][adj]) >= MAX_SENTENCES_PER_ADJ: continue\n",
    "                        # check if the adj word appears in the sentence\n",
    "                        if words.count(adj) != 1: continue\n",
    "                        # cleaning method 1: part-of-speech check - make sure the POS of word we found in the sentence is indeed adjective\n",
    "                        words_pos = nltk.pos_tag(words)\n",
    "                        adj_pos = next(x for x in words_pos if x[0] == adj)[1]\n",
    "                        if adj_pos != \"JJ\": continue # JJ = adjective\n",
    "                        # add sentence to adj dictionary\n",
    "                        if not adj in scale[dataset_type]: scale[dataset_type][adj] = []\n",
    "                        scale[dataset_type][adj].append(sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8336448"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# save data to json\n",
    "open(\"data.json\", mode='w', encoding=\"utf-8\").write(json.dumps(data, sort_keys=True, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need this for adjective substitution in sentences. unfortunately nltk doesn't provide this function\n",
    "def nltk_word_detokenize(tokens):\n",
    "    # Join tokens with spaces and handle punctuation\n",
    "    detokenized = ' '.join(tokens)\n",
    "    detokenized = detokenized.replace(\" ,\", \",\")\n",
    "    detokenized = detokenized.replace(\" .\", \".\")\n",
    "    detokenized = detokenized.replace(\" !\", \"!\")\n",
    "    detokenized = detokenized.replace(\" ?\", \"?\")\n",
    "    detokenized = detokenized.replace(\" :\", \":\")\n",
    "    detokenized = detokenized.replace(\" ;\", \";\")\n",
    "    # Add space after commas and periods if missing\n",
    "    detokenized = detokenized.replace(\",\", \", \")\n",
    "    detokenized = detokenized.replace(\".\", \". \")\n",
    "    return detokenized.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleaning sentences from adjectives in dataset 'demelo'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 87/87 [15:41<00:00, 10.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleaning sentences from adjectives in dataset 'crowd'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [12:32<00:00,  9.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleaning sentences from adjectives in dataset 'wilkinson'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21/21 [04:17<00:00, 12.28s/it]\n"
     ]
    }
   ],
   "source": [
    "## apply cleaning methods 2,3 on parsed sentences\n",
    "from transformers import BertModel\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "torch.set_grad_enabled(False)\n",
    "tokenizer = cola_tokenizer # we use the same tokenizer (BERT's) for both cleaning method\n",
    "bert = BertModel.from_pretrained(\"bert-base-uncased\").to('cuda')\n",
    "bert.eval()\n",
    "MAX_SENTENCES_PER_SCALE = 20\n",
    "\n",
    "# clean our data (get rid of most sentences)\n",
    "for scales_dataset_name in data.keys(): # 'demelo' / 'crowd' / 'wilkinson'\n",
    "    print(f\"cleaning sentences from adjectives in dataset '{scales_dataset_name}'...\")\n",
    "    for scale in tqdm(data[scales_dataset_name].values()): # '{ \"adjectives\": [\"nearby || near\", \"close\"] }'\n",
    "        for dataset_type in ['reviews', 'news']:\n",
    "            if \"sentences_similarities\" not in scale[dataset_type]: scale[dataset_type][\"sentences_similarities\"] = {}\n",
    "            adjs = extract_adjectives(scale['adjectives']) # flatten list of adjectives as it might contain ties\n",
    "            for adj in adjs:\n",
    "                if not adj in scale[dataset_type]: continue\n",
    "                adj_word_token = nltk.word_tokenize(adj)[0]\n",
    "                other_adjs = [x for x in adjs if x != adj]\n",
    "                other_adjs_word_tokens = [nltk.word_tokenize(x)[0] for x in other_adjs]\n",
    "                \n",
    "                sentences = scale[dataset_type][adj]\n",
    "                for sentence in sentences:\n",
    "                    # generate new sentences with substituted adjectives\n",
    "                    sentence_word_tokens = nltk.word_tokenize(sentence)\n",
    "                    all_sentences = [sentence_word_tokens] # original sentence is the first element\n",
    "                    for other_adj_word_token in other_adjs_word_tokens:\n",
    "                        all_sentences.append([other_adj_word_token if x==adj_word_token else x for x in sentence_word_tokens])\n",
    "                    all_sentences = [nltk_word_detokenize(x) for x in all_sentences]\n",
    "\n",
    "                    # cleaning meathod 2: check if sentences (original sentence + and generated sentences) are grammatically correct using a bert model finetuned on cola from earlier\n",
    "                    sentences_tokens = tokenizer(all_sentences, return_tensors=\"pt\", padding=True, truncation=True).to('cuda')\n",
    "                    cola_model_output = cola_model(**sentences_tokens)\n",
    "                    logits = cola_model_output.logits.cpu().numpy()\n",
    "                    predictions = np.argmax(logits, axis=-1)\n",
    "                    if 0 in predictions: # 0 means we predicted the sentence to be not grammatically correct\n",
    "                        continue # skip this sentence\n",
    "\n",
    "                    # cleaning method 3: compare similarities of generated sentences to the original sentence, and take top k sentences\n",
    "                    last_hidden_state = bert(**sentences_tokens).last_hidden_state\n",
    "                    # calculate similarities of the original sentence's embedding with the substituted sentences embeddings\n",
    "                    # use the embedding of BERT's CLS token as a represenation for the full sequence\n",
    "                    original_embedding = last_hidden_state[0,0,:] # original sentence is the first in the batch\n",
    "                    substituted_embeddings = last_hidden_state[1:,0,:]\n",
    "                    similarities = [F.cosine_similarity(original_embedding, x, dim=0) for x in substituted_embeddings]\n",
    "                    similarity = torch.mean(torch.tensor(similarities))\n",
    "                    \n",
    "                    # save similarity of this sentence with the original adjective replaced by _MASK_\n",
    "                    sentence_with_mask_token = nltk_word_detokenize([\"_MASK_\" if x==adj_word_token else x for x in sentence_word_tokens])\n",
    "                    scale[dataset_type][\"sentences_similarities\"][sentence_with_mask_token] = similarity.item()\n",
    "                \n",
    "            # get top k sentences with higest score for this datatype\n",
    "            top_k_sentences = [x[0] for x in sorted(scale[dataset_type][\"sentences_similarities\"].items(), key=lambda x: x[1], reverse=True)[:MAX_SENTENCES_PER_SCALE]]\n",
    "            scale[dataset_type][\"top_sentences\"] = top_k_sentences\n",
    "            # clean remaining sentences for all adjectives\n",
    "            for adj in adjs:\n",
    "                if adj in scale[dataset_type]: del scale[dataset_type][adj]\n",
    "            del scale[dataset_type]['sentences_similarities']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1235124"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "open(\"data_cleaned.json\", mode='w', encoding=\"utf-8\").write(json.dumps(data, sort_keys=True, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the tokens of a specific adjective a from a sequence\n",
    "# we need this because a single word may sometimes get tokenized into multiple tokens\n",
    "def find_sub_list(l,sl) -> tuple[int,int]:\n",
    "    sll=len(sl)\n",
    "    for ind in (i for i,e in enumerate(l) if e==sl[0]):\n",
    "        if l[ind:ind+sll]==sl:\n",
    "            return ind,ind+sll-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating 'bert-base-uncased' on ranking method 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating on dataset 'demelo'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 87/87 [01:01<00:00,  1.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating on dataset 'crowd'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:29<00:00,  2.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating on dataset 'wilkinson'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21/21 [00:09<00:00,  2.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating 'distilbert-base-uncased' on ranking method 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating on dataset 'demelo'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 87/87 [00:31<00:00,  2.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating on dataset 'crowd'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:15<00:00,  5.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating on dataset 'wilkinson'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21/21 [00:04<00:00,  4.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating 'albert-base-v2' on ranking method 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at albert-base-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.dense.bias', 'predictions.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.decoder.bias']\n",
      "- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating on dataset 'demelo'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 87/87 [01:03<00:00,  1.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating on dataset 'crowd'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:32<00:00,  2.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating on dataset 'wilkinson'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21/21 [00:10<00:00,  2.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating 'google/electra-base-generator' on ranking method 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/electra-base-generator were not used when initializing ElectraModel: ['generator_predictions.LayerNorm.bias', 'generator_predictions.dense.bias', 'generator_lm_head.weight', 'generator_lm_head.bias', 'generator_predictions.dense.weight', 'generator_predictions.LayerNorm.weight']\n",
      "- This IS expected if you are initializing ElectraModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating on dataset 'demelo'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 87/87 [00:32<00:00,  2.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating on dataset 'crowd'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:16<00:00,  4.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating on dataset 'wilkinson'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21/21 [00:05<00:00,  3.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating 'roberta-base' on ranking method 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating on dataset 'demelo'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 87/87 [01:01<00:00,  1.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating on dataset 'crowd'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:29<00:00,  2.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating on dataset 'wilkinson'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21/21 [00:10<00:00,  2.08it/s]\n"
     ]
    }
   ],
   "source": [
    "## rank scalar adjectives with method 1\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# compare BERT, DistilBERT, ALBERT, ELECTRA, RoBERTa\n",
    "# note that for electra we used the generator model as it was trained on word replacement task, which is in the nature of this project\n",
    "model_names = [\"bert-base-uncased\", \"distilbert-base-uncased\", \"albert-base-v2\", \"google/electra-base-generator\", \"roberta-base\"]\n",
    "\n",
    "for model_name in model_names:\n",
    "    print(f\"evaluating '{model_name}' on ranking method 1\")\n",
    "    model_specific_args = { 'add_prefix_space': True } if model_name == \"roberta-base\" else {} # Roberta tokenizes words differently whether they are in the beginning or not, so pass this\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, **model_specific_args)\n",
    "    model = AutoModel.from_pretrained(model_name).to(\"cuda\")\n",
    "    model.eval()\n",
    "    \n",
    "    for scales_dataset_name in data.keys(): # 'demelo' / 'crowd' / 'wilkinson'\n",
    "        print(f\"evaluating on dataset '{scales_dataset_name}'...\")\n",
    "        for scale in tqdm(data[scales_dataset_name].values()): # '{ \"adjectives\": [\"nearby || near\", \"close\"] }'\n",
    "            adjs = extract_adjectives(scale[\"adjectives\"]) # flatten list of adjectives as it might contain ties\n",
    "            for dataset_type in ['reviews', 'news']:\n",
    "                top_sentences = scale[dataset_type][\"top_sentences\"]\n",
    "                if len(top_sentences) == 0: continue\n",
    "                if len(adjs) <= 2 : continue # we need at least 3 adjective in the scale for this method\n",
    "                adjs_tokens_ids = [tokenizer.convert_tokens_to_ids(tokenizer.tokenize(adj)) for adj in adjs]\n",
    "                sum_similarities = torch.zeros(len(adjs)-1)\n",
    "                \n",
    "                for sentence in top_sentences:\n",
    "                    adj_sentences = [sentence.replace(\"_MASK_\", adj) for adj in adjs]\n",
    "                    tokens = tokenizer(adj_sentences, return_tensors=\"pt\", padding=True, truncation=True).to(\"cuda\")\n",
    "                    last_hidden_state = model(**tokens).last_hidden_state\n",
    "                    # extract embeddings of the adjectives\n",
    "                    tokens_input_ids = tokens[\"input_ids\"].tolist()\n",
    "                    adjs_ids_indices = [find_sub_list(all_tok_ids, adj_ids) for all_tok_ids, adj_ids in zip(tokens_input_ids, adjs_tokens_ids)]\n",
    "                    adjs_embeddings = [embs[adj_indices[0]:adj_indices[1]+1] for embs, adj_indices in zip(last_hidden_state, adjs_ids_indices)]\n",
    "                    # average embeddings in the case of a word having multiple tokens\n",
    "                    adjs_averaged_embeddings = [torch.mean(emb, dim=0) for emb in adjs_embeddings]\n",
    "                    # compute similarities of all adjectives with the last adjective, which is the extreme adjective in the list\n",
    "                    similarities = [F.cosine_similarity(emb, adjs_averaged_embeddings[-1], dim=0) for emb in adjs_averaged_embeddings[:-1]]\n",
    "                    sum_similarities += (torch.tensor(similarities) / torch.tensor(len(top_sentences), dtype=torch.float32))\n",
    "\n",
    "                scale[dataset_type][f\"eval_similarities_method1_{model_name}\"] = sum_similarities.tolist()\n",
    "                    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating 'bert-base-uncased' on ranking method 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset demelo contains 87 scales, splitting to train/test\n",
      "calculating intensity vectors (1 vec for 'reviews', 1 vec for 'news') for dataset 'demelo' on 60 train scales\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:37<00:00,  1.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating method 2 on intensity vectors for dataset 'demelo' on 27 test scales\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27/27 [00:20<00:00,  1.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset crowd contains 79 scales, splitting to train/test\n",
      "calculating intensity vectors (1 vec for 'reviews', 1 vec for 'news') for dataset 'crowd' on 55 train scales\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 55/55 [00:32<00:00,  1.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating method 2 on intensity vectors for dataset 'crowd' on 24 test scales\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:17<00:00,  1.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset wilkinson contains 21 scales, splitting to train/test\n",
      "calculating intensity vectors (1 vec for 'reviews', 1 vec for 'news') for dataset 'wilkinson' on 14 train scales\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:08<00:00,  1.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating method 2 on intensity vectors for dataset 'wilkinson' on 7 test scales\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:04<00:00,  1.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating 'distilbert-base-uncased' on ranking method 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset demelo contains 87 scales, splitting to train/test\n",
      "calculating intensity vectors (1 vec for 'reviews', 1 vec for 'news') for dataset 'demelo' on 60 train scales\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:18<00:00,  3.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating method 2 on intensity vectors for dataset 'demelo' on 27 test scales\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27/27 [00:10<00:00,  2.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset crowd contains 79 scales, splitting to train/test\n",
      "calculating intensity vectors (1 vec for 'reviews', 1 vec for 'news') for dataset 'crowd' on 55 train scales\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 55/55 [00:19<00:00,  2.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating method 2 on intensity vectors for dataset 'crowd' on 24 test scales\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:09<00:00,  2.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset wilkinson contains 21 scales, splitting to train/test\n",
      "calculating intensity vectors (1 vec for 'reviews', 1 vec for 'news') for dataset 'wilkinson' on 14 train scales\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:04<00:00,  2.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating method 2 on intensity vectors for dataset 'wilkinson' on 7 test scales\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:02<00:00,  2.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating 'albert-base-v2' on ranking method 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at albert-base-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.dense.bias', 'predictions.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.decoder.bias']\n",
      "- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset demelo contains 87 scales, splitting to train/test\n",
      "calculating intensity vectors (1 vec for 'reviews', 1 vec for 'news') for dataset 'demelo' on 60 train scales\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:39<00:00,  1.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating method 2 on intensity vectors for dataset 'demelo' on 27 test scales\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27/27 [00:21<00:00,  1.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset crowd contains 79 scales, splitting to train/test\n",
      "calculating intensity vectors (1 vec for 'reviews', 1 vec for 'news') for dataset 'crowd' on 55 train scales\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 55/55 [00:35<00:00,  1.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating method 2 on intensity vectors for dataset 'crowd' on 24 test scales\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:18<00:00,  1.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset wilkinson contains 21 scales, splitting to train/test\n",
      "calculating intensity vectors (1 vec for 'reviews', 1 vec for 'news') for dataset 'wilkinson' on 14 train scales\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:09<00:00,  1.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating method 2 on intensity vectors for dataset 'wilkinson' on 7 test scales\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:05<00:00,  1.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating 'google/electra-base-generator' on ranking method 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/electra-base-generator were not used when initializing ElectraModel: ['generator_predictions.LayerNorm.bias', 'generator_predictions.dense.bias', 'generator_lm_head.weight', 'generator_lm_head.bias', 'generator_predictions.dense.weight', 'generator_predictions.LayerNorm.weight']\n",
      "- This IS expected if you are initializing ElectraModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset demelo contains 87 scales, splitting to train/test\n",
      "calculating intensity vectors (1 vec for 'reviews', 1 vec for 'news') for dataset 'demelo' on 60 train scales\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:25<00:00,  2.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating method 2 on intensity vectors for dataset 'demelo' on 27 test scales\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27/27 [00:11<00:00,  2.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset crowd contains 79 scales, splitting to train/test\n",
      "calculating intensity vectors (1 vec for 'reviews', 1 vec for 'news') for dataset 'crowd' on 55 train scales\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 55/55 [00:23<00:00,  2.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating method 2 on intensity vectors for dataset 'crowd' on 24 test scales\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:10<00:00,  2.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset wilkinson contains 21 scales, splitting to train/test\n",
      "calculating intensity vectors (1 vec for 'reviews', 1 vec for 'news') for dataset 'wilkinson' on 14 train scales\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:05<00:00,  2.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating method 2 on intensity vectors for dataset 'wilkinson' on 7 test scales\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:03<00:00,  2.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating 'roberta-base' on ranking method 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset demelo contains 87 scales, splitting to train/test\n",
      "calculating intensity vectors (1 vec for 'reviews', 1 vec for 'news') for dataset 'demelo' on 60 train scales\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:37<00:00,  1.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating method 2 on intensity vectors for dataset 'demelo' on 27 test scales\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27/27 [00:21<00:00,  1.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset crowd contains 79 scales, splitting to train/test\n",
      "calculating intensity vectors (1 vec for 'reviews', 1 vec for 'news') for dataset 'crowd' on 55 train scales\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 55/55 [00:34<00:00,  1.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating method 2 on intensity vectors for dataset 'crowd' on 24 test scales\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:17<00:00,  1.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset wilkinson contains 21 scales, splitting to train/test\n",
      "calculating intensity vectors (1 vec for 'reviews', 1 vec for 'news') for dataset 'wilkinson' on 14 train scales\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:09<00:00,  1.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating method 2 on intensity vectors for dataset 'wilkinson' on 7 test scales\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:04<00:00,  1.46it/s]\n"
     ]
    }
   ],
   "source": [
    "## rank scalar adjectives with method 2\n",
    "TRAIN_SET_RATIO = 0.7\n",
    "\n",
    "for model_name in model_names:\n",
    "    print(f\"evaluating '{model_name}' on ranking method 2\")\n",
    "    model_specific_args = { 'add_prefix_space': True } if model_name == \"roberta-base\" else {} # Roberta tokenizes words differently whether they are in the beginning or not, so pass this\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, **model_specific_args)\n",
    "    model = AutoModel.from_pretrained(model_name).to(\"cuda\")\n",
    "    model_hidden_size = model.config.hidden_size\n",
    "    model.eval()\n",
    "    \n",
    "    for scales_dataset_name in data.keys(): # 'demelo' / 'crowd' / 'wilkinson'\n",
    "        scales_dataset = data[scales_dataset_name]\n",
    "        print(f\"dataset {scales_dataset_name} contains {len(scales_dataset.values())} scales, splitting to train/test\")\n",
    "        train_set, test_set = np.split(list(scales_dataset.values()), [int(len(scales_dataset.values())*TRAIN_SET_RATIO)])\n",
    "        print(f\"calculating intensity vectors (1 vec for 'reviews', 1 vec for 'news') for dataset '{scales_dataset_name}' on {len(train_set)} train scales\")\n",
    "        scales_dataset_diff_vecs = torch.zeros([2, model_hidden_size]) # 2 vecs for 'reviews' and 'news'\n",
    "\n",
    "        for scale in tqdm(train_set): # '{ \"adjectives\": [\"nearby || near\", \"close\"] }'\n",
    "            adjs = extract_adjectives(scale[\"adjectives\"])\n",
    "            adjs = [adjs[0], adjs[-1]] # we only look at the mild intensity adjective and the extreme intensity adjective\n",
    "            adjs_tokens_ids = [tokenizer.convert_tokens_to_ids(tokenizer.tokenize(adj)) for adj in adjs]\n",
    "            scale_diff_vecs = torch.zeros([2, model_hidden_size]) # for 'reviews' and 'news'\n",
    "            for dataset_type_index, dataset_type in enumerate(['reviews', 'news']):\n",
    "                top_sentences = scale[dataset_type][\"top_sentences\"]\n",
    "                if len(top_sentences) == 0: continue\n",
    "                for sentence in top_sentences:\n",
    "                    adj_sentences = [sentence.replace(\"_MASK_\", adj) for adj in adjs]\n",
    "                    tokens = tokenizer(adj_sentences, return_tensors=\"pt\", padding=True, truncation=True).to(\"cuda\")\n",
    "                    last_hidden_state = model(**tokens).last_hidden_state\n",
    "                    # extract embeddings of the adjectives\n",
    "                    tokens_input_ids = tokens[\"input_ids\"].tolist()\n",
    "                    adjs_ids_indices = [find_sub_list(all_tok_ids, adj_ids) for all_tok_ids, adj_ids in zip(tokens_input_ids, adjs_tokens_ids)]\n",
    "                    adjs_embeddings = [embs[adj_indices[0]:adj_indices[1]+1] for embs, adj_indices in zip(last_hidden_state, adjs_ids_indices)]\n",
    "                    # average embeddings in the case of a word having multiple tokens\n",
    "                    adjs_averaged_embeddings = [torch.mean(emb, dim=0) for emb in adjs_embeddings]\n",
    "                    # compute diff vec\n",
    "                    sentence_diff = adjs_averaged_embeddings[1] - adjs_averaged_embeddings[0]\n",
    "                    scale_diff_vecs[dataset_type_index] += (sentence_diff.to('cpu') / torch.tensor(len(top_sentences), dtype=torch.float32))\n",
    "            scales_dataset_diff_vecs += (scale_diff_vecs / torch.tensor(len(train_set), dtype=torch.float32))\n",
    "        \n",
    "        print(f\"evaluating method 2 on intensity vectors for dataset '{scales_dataset_name}' on {len(test_set)} test scales\")\n",
    "        for scale in tqdm(test_set): # '{ \"adjectives\": [\"nearby || near\", \"close\"] }'\n",
    "            adjs = extract_adjectives(scale[\"adjectives\"])\n",
    "            for dataset_type_index, dataset_type in enumerate(['reviews', 'news']):\n",
    "                top_sentences = scale[dataset_type][\"top_sentences\"]\n",
    "                if len(top_sentences) == 0: continue\n",
    "                adjs_tokens_ids = [tokenizer.convert_tokens_to_ids(tokenizer.tokenize(adj)) for adj in adjs]\n",
    "                sum_similarities = torch.zeros(len(adjs))\n",
    "                \n",
    "                for sentence in top_sentences:\n",
    "                    adj_sentences = [sentence.replace(\"_MASK_\", adj) for adj in adjs]\n",
    "                    tokens = tokenizer(adj_sentences, return_tensors=\"pt\", padding=True, truncation=True).to(\"cuda\")\n",
    "                    last_hidden_state = model(**tokens).last_hidden_state\n",
    "                    # extract embeddings of the adjectives\n",
    "                    tokens_input_ids = tokens[\"input_ids\"].tolist()\n",
    "                    adjs_ids_indices = [find_sub_list(all_tok_ids, adj_ids) for all_tok_ids, adj_ids in zip(tokens_input_ids, adjs_tokens_ids)]\n",
    "                    adjs_embeddings = [embs[adj_indices[0]:adj_indices[1]+1] for embs, adj_indices in zip(last_hidden_state, adjs_ids_indices)]\n",
    "                    # average embeddings in the case of a word having multiple tokens\n",
    "                    adjs_averaged_embeddings = [torch.mean(emb, dim=0) for emb in adjs_embeddings]\n",
    "                    # compute similarities of all adjectives with the intensity vector\n",
    "                    similarities = [F.cosine_similarity(emb.to('cpu'), scales_dataset_diff_vecs[dataset_type_index], dim=0) for emb in adjs_averaged_embeddings]\n",
    "                    sum_similarities += (torch.tensor(similarities) / torch.tensor(len(top_sentences), dtype=torch.float32))\n",
    "\n",
    "                scale[dataset_type][f\"eval_adj_diffs_method2_{model_name}\"] = sum_similarities.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1507344"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "open(\"data_evaluated.json\", mode='w', encoding=\"utf-8\").write(json.dumps(data, sort_keys=True, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing results to 'results.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6626"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## calculate and results to 'results.json'\n",
    "import itertools\n",
    "\n",
    "METHOD1_PAIR_EQUALITY_RANGE = 0.0\n",
    "METHOD2_PAIR_EQUALITY_RANGE = 0.0\n",
    "\n",
    "results = {}\n",
    "\n",
    "for scales_dataset_name in data.keys(): # 'demelo' / 'crowd' / 'wilkinson'\n",
    "    results[scales_dataset_name] = {}\n",
    "    for dataset_type in ['reviews', 'news']:\n",
    "        results[scales_dataset_name][dataset_type] = {}\n",
    "        for model_name in model_names:\n",
    "            results[scales_dataset_name][dataset_type][model_name] = {}\n",
    "            for method in [\"method1\", \"method2\"]:\n",
    "                num_pairs = 0\n",
    "                num_correct_comparisons = 0\n",
    "\n",
    "                num_concordant_pairs = 0\n",
    "                num_discordant_pairs = 0\n",
    "                num_tied_pairs = 0\n",
    "                similarities_entry_name = f\"eval_similarities_method1_{model_name}\" if method == \"method1\" else f\"eval_adj_diffs_method2_{model_name}\"\n",
    "                pair_equality_range = METHOD1_PAIR_EQUALITY_RANGE if method == \"method1\" else METHOD2_PAIR_EQUALITY_RANGE\n",
    "                \n",
    "                for scale in data[scales_dataset_name].values():\n",
    "                    if not similarities_entry_name in scale[dataset_type]: continue\n",
    "                    adjs = extract_adjectives(scale['adjectives']) # flatten list of adjectives as it might contain ties\n",
    "                    if method == \"method1\": adjs = adjs[:-1]\n",
    "                    rankings = scale[dataset_type][similarities_entry_name]\n",
    "                    adj_rankings = list(zip(adjs, rankings))\n",
    "                    all_possible_pairs = itertools.combinations(adj_rankings, 2)\n",
    "                    for pair in all_possible_pairs:\n",
    "                        gold_truth = \"\"\n",
    "                        pair0_name, pair0_rank_pred = pair[0]\n",
    "                        pair1_name, pair1_rank_pred = pair[1]\n",
    "                        adjectives_full_list = scale['adjectives'] # full list with ties (e.g. [\"small || little\", \"tiny\"])\n",
    "                        pair0_index = [i for i, x in enumerate(adjectives_full_list) if pair0_name in x.split(\" || \")]\n",
    "                        pair1_index = [i for i, x in enumerate(adjectives_full_list) if pair1_name in x.split(\" || \")]\n",
    "                        assert len(pair0_index) == len(pair1_index) == 1\n",
    "                        pair0_index = pair0_index[0]\n",
    "                        pair1_index = pair1_index[0]\n",
    "\n",
    "                        if pair1_index > pair0_index:\n",
    "                            gold_truth = \"bigger\"\n",
    "                        elif pair1_index < pair0_index:\n",
    "                            gold_truth = \"smaller\"\n",
    "                        elif pair0_index == pair1_index and \"||\" in adjectives_full_list[pair0_index]:\n",
    "                            gold_truth = \"equal\"\n",
    "                        assert gold_truth != \"\"\n",
    "\n",
    "                        pair_pred_diff = pair1_rank_pred - pair0_rank_pred\n",
    "                        pair_pred = \"equal\" if abs(pair_pred_diff) <= pair_equality_range else \"bigger\" if pair_pred_diff > 0 else \"smaller\"\n",
    "                        # pairwise\n",
    "                        if pair_pred == gold_truth: num_correct_comparisons += 1\n",
    "                        # kendall\n",
    "                        if pair_pred == gold_truth and (gold_truth == \"bigger\" or gold_truth == \"smaller\"):\n",
    "                            num_concordant_pairs += 1\n",
    "                        elif pair_pred == gold_truth:\n",
    "                            num_tied_pairs +=1\n",
    "                        else:\n",
    "                            num_discordant_pairs += 1\n",
    "                        \n",
    "                        num_pairs += 1\n",
    "                    \n",
    "                results[scales_dataset_name][dataset_type][model_name][method] = {\n",
    "                    \"pairwise\": num_correct_comparisons / num_pairs,\n",
    "                    \"kendall\": (num_concordant_pairs - num_discordant_pairs) / num_pairs\n",
    "                }\n",
    "\n",
    "# round floats to 3 digits after dot\n",
    "def round_floats(o):\n",
    "    if isinstance(o, float): return round(o, 3)\n",
    "    if isinstance(o, dict): return {k: round_floats(v) for k, v in o.items()}\n",
    "    if isinstance(o, (list, tuple)): return [round_floats(x) for x in o]\n",
    "    return o\n",
    "\n",
    "print(\"writing results to 'results.json'\")\n",
    "open(\"results.json\", mode='w', encoding=\"utf-8\").write(json.dumps(round_floats(results), sort_keys=True, indent=2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
